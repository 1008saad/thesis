{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3 VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data into numpy array\n",
    "\n",
    "\n",
    "Below is the code for loading the image data from the train, test, and validation folders into numpy arrays, and performing normalization of every pixel in the range of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "data_dir = \"F:/thesis/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Get the total number of images\n",
    "    num_images = sum(len(files) for _, _, files in os.walk(dataset_dir))\n",
    "\n",
    "    # Create tqdm progress bar\n",
    "    pbar = tqdm(total=num_images, desc=f'Loading {dataset_dir}', unit='image')\n",
    "\n",
    "    # Iterate through each subfolder in the dataset directory\n",
    "    for class_folder in sorted(os.listdir(dataset_dir)):\n",
    "        class_dir = os.path.join(dataset_dir, class_folder)\n",
    "        if os.path.isdir(class_dir):\n",
    "            # Iterate through each image file in the class folder\n",
    "            for image_file in sorted(os.listdir(class_dir)):\n",
    "                image_path = os.path.join(class_dir, image_file)\n",
    "                # Load image using PIL\n",
    "                image = Image.open(image_path)\n",
    "                # Resize image to 224x224 if necessary (optional)\n",
    "                image = image.resize((224, 224))\n",
    "                # Convert image to numpy array and normalize pixel values\n",
    "                image = np.array(image) / 255.0\n",
    "                # Append image and corresponding label to lists\n",
    "                images.append(image)\n",
    "                labels.append(int(class_folder))\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Close progress bar after completion\n",
    "    pbar.close()\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load data for training set\n",
    "train_images, train_labels = load_data(os.path.join(data_dir, 'train'))\n",
    "\n",
    "# Load data for test set\n",
    "test_images, test_labels = load_data(os.path.join(data_dir, 'test'))\n",
    "\n",
    "# Load data for validation set\n",
    "val_images, val_labels = load_data(os.path.join(data_dir, 'validation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train images array:\", train_images.shape)\n",
    "print(\"Shape of train labels array:\", train_labels.shape)\n",
    "print(\"Shape of test images array:\", test_images.shape)\n",
    "print(\"Shape of test labels array:\", test_labels.shape)\n",
    "print(\"Shape of validation images array:\", val_images.shape)\n",
    "print(\"Shape of validation labels array:\", val_labels.shape)\n",
    "print(train_images)\n",
    "print(type(train_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layer with 32 filters, kernel size of 3x3, and ReLU activation function\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "\n",
    "# Add max pooling layer with pool size of 2x2\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Add another convolutional layer with 64 filters, kernel size of 3x3, and ReLU activation function\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add another max pooling layer with pool size of 2x2\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output from the previous layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected (dense) layer with 128 neurons and ReLU activation function\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add dropout layer with dropout rate of 0.5 to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add output layer with softmax activation function for multi-class classification\n",
    "num_classes = 38  # Change this according to the number of classes in your dataset\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded your data into variables train_images, train_labels, test_images, test_labels, val_images, val_labels\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    batch_size=32,\n",
    "                    epochs=50,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict labels for the validation set\n",
    "val_predictions = model.predict(val_images)\n",
    "val_predictions = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Calculate relevant metrics\n",
    "report = classification_report(val_labels, val_predictions)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "test_labels_one_hot = to_categorical(test_labels)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "test_probabilities = model.predict(test_images)\n",
    "\n",
    "# Compute AUC-ROC for each class\n",
    "auc_roc_scores = []\n",
    "for i in range(num_classes):\n",
    "    auc = roc_auc_score(test_labels_one_hot[:, i], test_probabilities[:, i])\n",
    "    auc_roc_scores.append(auc)\n",
    "\n",
    "# Print AUC-ROC scores for each class\n",
    "for i, auc in enumerate(auc_roc_scores):\n",
    "    print(f\"Class {i}: AUC-ROC = {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
