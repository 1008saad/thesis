{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># Ensemble Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip sign.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "stg=tf.distribute.MirroredStrategy(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "folder_dir = 'F:/thesis/data'\n",
    "# SIZE = 224\n",
    "# DOWNSAMPLE_RATIO = 4\n",
    "# JPEG_QUALITY = 100\n",
    "\n",
    "total_files = sum(len(files) for _, _, files in os.walk(folder_dir))\n",
    "\n",
    "with tqdm(total=total_files, desc=\"Processing Images\") as pbar:\n",
    "    for folder in os.listdir(folder_dir):\n",
    "        for file in os.listdir(os.path.join(folder_dir, folder)):\n",
    "                image_path = os.path.join(folder_dir, folder, file)\n",
    "                img = cv2.imread(image_path)\n",
    "                # img_resized = cv2.resize(img, (SIZE,SIZE))\n",
    "                # cv2.imwrite(image_path, img_resized)\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "picture_size = (224, 224)\n",
    "train_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=folder_dir,\n",
    "    shuffle=True,\n",
    "    image_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed = 22\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=folder_dir,\n",
    "    shuffle=True,\n",
    "    image_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed = 22\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.layers import RandomRotation, RandomTranslation, RandomFlip, RandomZoom\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_classes = 38\n",
    "\n",
    "# Define the Xception base model\n",
    "base_model = Xception(weights=\"imagenet\", include_top=False, input_shape=(SIZE, SIZE, 3))\n",
    "\n",
    "# Create three instances of the base model with different inputs\n",
    "input_1 = base_model.input\n",
    "output_1 = base_model.output\n",
    "input_2 = base_model.input\n",
    "output_2 = base_model.output\n",
    "input_3 = base_model.input\n",
    "output_3 = base_model.output\n",
    "\n",
    "# Add global average pooling layer and dense layers for each model\n",
    "output_1 = GlobalAveragePooling2D()(output_1)\n",
    "output_1 = Dense(512, activation='relu')(output_1)\n",
    "output_1 = Dropout(0.5)(output_1)\n",
    "output_1 = Dense(32, activation='relu')(output_1)\n",
    "output_1 = Dense(no_of_classes, activation='softmax')(output_1)\n",
    "\n",
    "output_2 = GlobalAveragePooling2D()(output_2)\n",
    "output_2 = Dense(512, activation='relu')(output_2)\n",
    "output_2 = Dropout(0.5)(output_2)\n",
    "output_2 = Dense(32, activation='relu')(output_2)\n",
    "output_2 = Dense(no_of_classes, activation='softmax')(output_2)\n",
    "\n",
    "output_3 = GlobalAveragePooling2D()(output_3)\n",
    "output_3 = Dense(512, activation='relu')(output_3)\n",
    "output_3 = Dropout(0.5)(output_3)\n",
    "output_3 = Dense(32, activation='relu')(output_3)\n",
    "output_3 = Dense(no_of_classes, activation='softmax')(output_3)\n",
    "\n",
    "# Create three separate models\n",
    "model_1 = Model(inputs=input_1, outputs=output_1)\n",
    "model_2 = Model(inputs=input_2, outputs=output_2)\n",
    "model_3 = Model(inputs=input_3, outputs=output_3)\n",
    "\n",
    "# Compile the models\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 factor=0.1,\n",
    "                                 patience=2,\n",
    "                                 verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                patience=2,\n",
    "                                verbose=1)\n",
    "\n",
    "callbacks = [lr_scheduler, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model 1\n",
    "model_1.fit(train_set, epochs=10, validation_data=validation_set, callbacks=callbacks,\n",
    "                    steps_per_epoch=len(train_set), validation_steps=len(validation_set))\n",
    "\n",
    "# Train model 2\n",
    "model_2.fit(train_set, epochs=10, validation_data=validation_set, callbacks=callbacks,\n",
    "                    steps_per_epoch=len(train_set), validation_steps=len(validation_set))\n",
    "\n",
    "# Train model 3\n",
    "model_3.fit(train_set, epochs=10, validation_data=validation_set, callbacks=callbacks,\n",
    "                    steps_per_epoch=len(train_set), validation_steps=len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "num_models = 3\n",
    "test_data_dir = '/content/RESIZED_TESTING_DATA'\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "# Generate predictions for each model and collect them in a list\n",
    "for model in [model_1, model_2, model_3]:\n",
    "    predictions = model.predict_generator(test_generator)\n",
    "    ensemble_predictions.append(predictions)\n",
    "\n",
    "# Compute the average predictions of the ensemble\n",
    "ensemble_predictions = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "# Calculate the accuracy of the ensemble predictions\n",
    "ensemble_acc = np.mean(np.argmax(ensemble_predictions, axis=1) == test_generator.classes)\n",
    "\n",
    "print('Ensemble accuracy:', ensemble_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
