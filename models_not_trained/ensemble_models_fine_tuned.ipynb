{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Ensemble Xception+InceptionV3+ResNet50+EfficientNetB0\n",
    "Fine Tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Colab uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !unzip \"/content/drive/MyDrive/Dataset/data.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = '/content/data'\n",
    "no_of_classes = 38\n",
    "batch_size = 32\n",
    "size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "stg=tf.distribute.MirroredStrategy(gpus)\n",
    "\n",
    "folder_dir = location\n",
    "total_files = sum(len(files) for _, _, files in os.walk(folder_dir))\n",
    "\n",
    "with tqdm(total=total_files, desc=\"Processing Images\") as pbar:\n",
    "    for folder in os.listdir(folder_dir):\n",
    "        for file in os.listdir(os.path.join(folder_dir, folder)):\n",
    "            image_path = os.path.join(folder_dir, folder, file)\n",
    "            img = cv2.imread(image_path)\n",
    "            img_resized = cv2.resize(img, (size, size))\n",
    "            cv2.imwrite(image_path, img_resized)\n",
    "            pbar.update(1)\n",
    "picture_size = (size, size)\n",
    "train_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=folder_dir,\n",
    "    shuffle=True,\n",
    "    image_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=22\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=folder_dir,\n",
    "    shuffle=True,\n",
    "    image_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build pre-trained models\n",
    "def build_model(base_model):    \n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(38, activation='softmax')(x)  # Assuming 38 classes\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(lr=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load pre-trained models\n",
    "models = [\n",
    "    Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
    "    InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
    "    ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
    "    EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "]\n",
    "\n",
    "# Build and compile models\n",
    "ensemble_models = [build_model(model) for model in models]\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 factor=0.1,\n",
    "                                 patience=2,\n",
    "                                 verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=5,\n",
    "                               verbose=1)\n",
    "\n",
    "callbacks = [lr_scheduler, early_stopping]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ensemble_models:\n",
    "    model.fit(train_set,\n",
    "              epochs=100,\n",
    "              validation_data=validation_set,\n",
    "              callbacks=callbacks,\n",
    "              steps_per_epoch=len(train_set),\n",
    "              validation_steps=len(validation_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "num_models = 3\n",
    "test_data_dir = location\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "# Generate predictions for each model and collect them in a list\n",
    "for model in [model_1, model_2, model_3]:\n",
    "    predictions = model.predict_generator(test_generator)\n",
    "    ensemble_predictions.append(predictions)\n",
    "\n",
    "# Compute the average predictions of the ensemble\n",
    "ensemble_predictions = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "# Calculate the accuracy of the ensemble predictions\n",
    "ensemble_acc = np.mean(np.argmax(ensemble_predictions, axis=1) == test_generator.classes)\n",
    "\n",
    "print('Ensemble accuracy:', ensemble_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
