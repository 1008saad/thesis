{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Xception\n",
    "Fine Tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Colab uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !unzip \"/content/drive/MyDrive/Dataset/data.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = '/content/data'\n",
    "no_of_classes = 38\n",
    "batch_size = 32\n",
    "size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "stg=tf.distribute.MirroredStrategy(gpus)\n",
    "\n",
    "folder_dir = location\n",
    "total_files = sum(len(files) for _, _, files in os.walk(folder_dir))\n",
    "\n",
    "with tqdm(total=total_files, desc=\"Processing Images\") as pbar:\n",
    "    for folder in os.listdir(folder_dir):\n",
    "        for file in os.listdir(os.path.join(folder_dir, folder)):\n",
    "            image_path = os.path.join(folder_dir, folder, file)\n",
    "            img = cv2.imread(image_path)\n",
    "            img_resized = cv2.resize(img, (size, size))\n",
    "            cv2.imwrite(image_path, img_resized)\n",
    "            pbar.update(1)\n",
    "picture_size = (size, size)\n",
    "train_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=folder_dir,\n",
    "    shuffle=True,\n",
    "    image_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=22\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=folder_dir,\n",
    "    shuffle=True,\n",
    "    image_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with stg.scope():\n",
    "    # Data Augmentation\n",
    "    img_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.2),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomTranslation(\n",
    "            height_factor=0.1, width_factor=0.1),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
    "            mode='horizontal'),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.2)\n",
    "    ], name=\"img_augmentation\")\n",
    "\n",
    "    base_model = Xception(weights=\"imagenet\", input_shape=(\n",
    "        size, size, 3), include_top=False)\n",
    "    inputs = Input(shape=(size, size, 3))\n",
    "    x = img_augmentation(inputs)\n",
    "    outputs = base_model(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(outputs)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    outputs = Dense(no_of_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 factor=0.1,\n",
    "                                 patience=2,\n",
    "                                 verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=5,\n",
    "                               verbose=1)\n",
    "\n",
    "callbacks = [lr_scheduler, early_stopping]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set, epochs=100, validation_data=validation_set, callbacks=callbacks,\n",
    "                    steps_per_epoch=len(train_set), validation_steps=len(validation_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = location\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=picture_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test set\n",
    "predictions = model.predict(test_generator)\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix using Seaborn\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Calculate overall ROC AUC score\n",
    "roc_auc = roc_auc_score(tf.keras.utils.to_categorical(y_true), predictions, average='macro')\n",
    "print(\"Overall ROC AUC Score:\", roc_auc)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(tf.keras.utils.to_categorical(y_true).ravel(), predictions.ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.5f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve EfficientNet')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
